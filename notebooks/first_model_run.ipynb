{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import functools\n",
    "\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "from Prithvi import MaskedAutoencoderViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file_path = '../Prithvi-100M/Prithvi_100M_config.yaml'\n",
    "with open(yaml_file_path, 'r') as f:\n",
    "    params = yaml.safe_load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ratio = None\n",
    "\n",
    "# data related\n",
    "num_frames = 3\n",
    "img_size = params['img_size']\n",
    "bands = params['bands']\n",
    "mean = params['data_mean']\n",
    "std = params['data_std']\n",
    "\n",
    "# model related\n",
    "depth = params['depth']\n",
    "patch_size = params['patch_size']\n",
    "embed_dim = params['embed_dim']\n",
    "num_heads = params['num_heads']\n",
    "tubelet_size = params['tubelet_size']\n",
    "decoder_embed_dim = params['decoder_embed_dim']\n",
    "decoder_num_heads = params['decoder_num_heads']\n",
    "decoder_depth = params['decoder_depth']\n",
    "\n",
    "batch_size = params['batch_size']\n",
    "\n",
    "mask_ratio = params['mask_ratio'] if mask_ratio is None else mask_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskedAutoencoderViT(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            num_frames=num_frames,\n",
    "            tubelet_size=tubelet_size,\n",
    "            in_chans=len(bands),\n",
    "            embed_dim=embed_dim,\n",
    "            depth=depth,\n",
    "            num_heads=num_heads,\n",
    "            decoder_embed_dim=decoder_embed_dim,\n",
    "            decoder_depth=decoder_depth,\n",
    "            decoder_num_heads=decoder_num_heads,\n",
    "            mlp_ratio=4.,\n",
    "            norm_layer=functools.partial(torch.nn.LayerNorm, eps=1e-6),\n",
    "            norm_pix_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> Model has 112,639,488 parameters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\n--> Model has {total_params:,} parameters.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Prithvi_100M.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/laszer/projects/nasa_hackathon/notebooks/first_model_run.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/laszer/projects/nasa_hackathon/notebooks/first_model_run.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mPrithvi_100M.pt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/laszer/projects/nasa_hackathon/notebooks/first_model_run.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(checkpoint, map_location\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/laszer/projects/nasa_hackathon/notebooks/first_model_run.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# discard fixed pos_embedding weight\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/laszer/projects/nasa_hackathon/notebooks/first_model_run.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdel\u001b[39;00m state_dict[\u001b[39m'\u001b[39m\u001b[39mpos_embed\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/hackathon_nasa/lib/python3.9/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/hackathon_nasa/lib/python3.9/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    436\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/hackathon_nasa/lib/python3.9/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Prithvi_100M.pt'"
     ]
    }
   ],
   "source": [
    "checkpoint = 'Prithvi_100M.pt'\n",
    "\n",
    "state_dict = torch.load(checkpoint, map_location=device)\n",
    "# discard fixed pos_embedding weight\n",
    "del state_dict['pos_embed']\n",
    "del state_dict['decoder_pos_embed']\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "print(f\"Loaded checkpoint from {checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 432.562MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedAutoencoderViT(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv3d(6, 768, kernel_size=(1, 16, 16), stride=(1, 16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (decoder_blocks): ModuleList(\n",
       "    (0-7): 8 x Block(\n",
       "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "  (decoder_pred): Linear(in_features=512, out_features=1536, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1       [-1, 768, 3, 14, 14]       1,180,416\n",
      "          Identity-2             [-1, 588, 768]               0\n",
      "        PatchEmbed-3             [-1, 588, 768]               0\n",
      "         LayerNorm-4             [-1, 148, 768]           1,536\n",
      "            Linear-5            [-1, 148, 2304]       1,771,776\n",
      "          Identity-6          [-1, 12, 148, 64]               0\n",
      "          Identity-7          [-1, 12, 148, 64]               0\n",
      "            Linear-8             [-1, 148, 768]         590,592\n",
      "           Dropout-9             [-1, 148, 768]               0\n",
      "        Attention-10             [-1, 148, 768]               0\n",
      "         Identity-11             [-1, 148, 768]               0\n",
      "         Identity-12             [-1, 148, 768]               0\n",
      "        LayerNorm-13             [-1, 148, 768]           1,536\n",
      "           Linear-14            [-1, 148, 3072]       2,362,368\n",
      "             GELU-15            [-1, 148, 3072]               0\n",
      "          Dropout-16            [-1, 148, 3072]               0\n",
      "         Identity-17            [-1, 148, 3072]               0\n",
      "           Linear-18             [-1, 148, 768]       2,360,064\n",
      "          Dropout-19             [-1, 148, 768]               0\n",
      "              Mlp-20             [-1, 148, 768]               0\n",
      "         Identity-21             [-1, 148, 768]               0\n",
      "         Identity-22             [-1, 148, 768]               0\n",
      "            Block-23             [-1, 148, 768]               0\n",
      "        LayerNorm-24             [-1, 148, 768]           1,536\n",
      "           Linear-25            [-1, 148, 2304]       1,771,776\n",
      "         Identity-26          [-1, 12, 148, 64]               0\n",
      "         Identity-27          [-1, 12, 148, 64]               0\n",
      "           Linear-28             [-1, 148, 768]         590,592\n",
      "          Dropout-29             [-1, 148, 768]               0\n",
      "        Attention-30             [-1, 148, 768]               0\n",
      "         Identity-31             [-1, 148, 768]               0\n",
      "         Identity-32             [-1, 148, 768]               0\n",
      "        LayerNorm-33             [-1, 148, 768]           1,536\n",
      "           Linear-34            [-1, 148, 3072]       2,362,368\n",
      "             GELU-35            [-1, 148, 3072]               0\n",
      "          Dropout-36            [-1, 148, 3072]               0\n",
      "         Identity-37            [-1, 148, 3072]               0\n",
      "           Linear-38             [-1, 148, 768]       2,360,064\n",
      "          Dropout-39             [-1, 148, 768]               0\n",
      "              Mlp-40             [-1, 148, 768]               0\n",
      "         Identity-41             [-1, 148, 768]               0\n",
      "         Identity-42             [-1, 148, 768]               0\n",
      "            Block-43             [-1, 148, 768]               0\n",
      "        LayerNorm-44             [-1, 148, 768]           1,536\n",
      "           Linear-45            [-1, 148, 2304]       1,771,776\n",
      "         Identity-46          [-1, 12, 148, 64]               0\n",
      "         Identity-47          [-1, 12, 148, 64]               0\n",
      "           Linear-48             [-1, 148, 768]         590,592\n",
      "          Dropout-49             [-1, 148, 768]               0\n",
      "        Attention-50             [-1, 148, 768]               0\n",
      "         Identity-51             [-1, 148, 768]               0\n",
      "         Identity-52             [-1, 148, 768]               0\n",
      "        LayerNorm-53             [-1, 148, 768]           1,536\n",
      "           Linear-54            [-1, 148, 3072]       2,362,368\n",
      "             GELU-55            [-1, 148, 3072]               0\n",
      "          Dropout-56            [-1, 148, 3072]               0\n",
      "         Identity-57            [-1, 148, 3072]               0\n",
      "           Linear-58             [-1, 148, 768]       2,360,064\n",
      "          Dropout-59             [-1, 148, 768]               0\n",
      "              Mlp-60             [-1, 148, 768]               0\n",
      "         Identity-61             [-1, 148, 768]               0\n",
      "         Identity-62             [-1, 148, 768]               0\n",
      "            Block-63             [-1, 148, 768]               0\n",
      "        LayerNorm-64             [-1, 148, 768]           1,536\n",
      "           Linear-65            [-1, 148, 2304]       1,771,776\n",
      "         Identity-66          [-1, 12, 148, 64]               0\n",
      "         Identity-67          [-1, 12, 148, 64]               0\n",
      "           Linear-68             [-1, 148, 768]         590,592\n",
      "          Dropout-69             [-1, 148, 768]               0\n",
      "        Attention-70             [-1, 148, 768]               0\n",
      "         Identity-71             [-1, 148, 768]               0\n",
      "         Identity-72             [-1, 148, 768]               0\n",
      "        LayerNorm-73             [-1, 148, 768]           1,536\n",
      "           Linear-74            [-1, 148, 3072]       2,362,368\n",
      "             GELU-75            [-1, 148, 3072]               0\n",
      "          Dropout-76            [-1, 148, 3072]               0\n",
      "         Identity-77            [-1, 148, 3072]               0\n",
      "           Linear-78             [-1, 148, 768]       2,360,064\n",
      "          Dropout-79             [-1, 148, 768]               0\n",
      "              Mlp-80             [-1, 148, 768]               0\n",
      "         Identity-81             [-1, 148, 768]               0\n",
      "         Identity-82             [-1, 148, 768]               0\n",
      "            Block-83             [-1, 148, 768]               0\n",
      "        LayerNorm-84             [-1, 148, 768]           1,536\n",
      "           Linear-85            [-1, 148, 2304]       1,771,776\n",
      "         Identity-86          [-1, 12, 148, 64]               0\n",
      "         Identity-87          [-1, 12, 148, 64]               0\n",
      "           Linear-88             [-1, 148, 768]         590,592\n",
      "          Dropout-89             [-1, 148, 768]               0\n",
      "        Attention-90             [-1, 148, 768]               0\n",
      "         Identity-91             [-1, 148, 768]               0\n",
      "         Identity-92             [-1, 148, 768]               0\n",
      "        LayerNorm-93             [-1, 148, 768]           1,536\n",
      "           Linear-94            [-1, 148, 3072]       2,362,368\n",
      "             GELU-95            [-1, 148, 3072]               0\n",
      "          Dropout-96            [-1, 148, 3072]               0\n",
      "         Identity-97            [-1, 148, 3072]               0\n",
      "           Linear-98             [-1, 148, 768]       2,360,064\n",
      "          Dropout-99             [-1, 148, 768]               0\n",
      "             Mlp-100             [-1, 148, 768]               0\n",
      "        Identity-101             [-1, 148, 768]               0\n",
      "        Identity-102             [-1, 148, 768]               0\n",
      "           Block-103             [-1, 148, 768]               0\n",
      "       LayerNorm-104             [-1, 148, 768]           1,536\n",
      "          Linear-105            [-1, 148, 2304]       1,771,776\n",
      "        Identity-106          [-1, 12, 148, 64]               0\n",
      "        Identity-107          [-1, 12, 148, 64]               0\n",
      "          Linear-108             [-1, 148, 768]         590,592\n",
      "         Dropout-109             [-1, 148, 768]               0\n",
      "       Attention-110             [-1, 148, 768]               0\n",
      "        Identity-111             [-1, 148, 768]               0\n",
      "        Identity-112             [-1, 148, 768]               0\n",
      "       LayerNorm-113             [-1, 148, 768]           1,536\n",
      "          Linear-114            [-1, 148, 3072]       2,362,368\n",
      "            GELU-115            [-1, 148, 3072]               0\n",
      "         Dropout-116            [-1, 148, 3072]               0\n",
      "        Identity-117            [-1, 148, 3072]               0\n",
      "          Linear-118             [-1, 148, 768]       2,360,064\n",
      "         Dropout-119             [-1, 148, 768]               0\n",
      "             Mlp-120             [-1, 148, 768]               0\n",
      "        Identity-121             [-1, 148, 768]               0\n",
      "        Identity-122             [-1, 148, 768]               0\n",
      "           Block-123             [-1, 148, 768]               0\n",
      "       LayerNorm-124             [-1, 148, 768]           1,536\n",
      "          Linear-125            [-1, 148, 2304]       1,771,776\n",
      "        Identity-126          [-1, 12, 148, 64]               0\n",
      "        Identity-127          [-1, 12, 148, 64]               0\n",
      "          Linear-128             [-1, 148, 768]         590,592\n",
      "         Dropout-129             [-1, 148, 768]               0\n",
      "       Attention-130             [-1, 148, 768]               0\n",
      "        Identity-131             [-1, 148, 768]               0\n",
      "        Identity-132             [-1, 148, 768]               0\n",
      "       LayerNorm-133             [-1, 148, 768]           1,536\n",
      "          Linear-134            [-1, 148, 3072]       2,362,368\n",
      "            GELU-135            [-1, 148, 3072]               0\n",
      "         Dropout-136            [-1, 148, 3072]               0\n",
      "        Identity-137            [-1, 148, 3072]               0\n",
      "          Linear-138             [-1, 148, 768]       2,360,064\n",
      "         Dropout-139             [-1, 148, 768]               0\n",
      "             Mlp-140             [-1, 148, 768]               0\n",
      "        Identity-141             [-1, 148, 768]               0\n",
      "        Identity-142             [-1, 148, 768]               0\n",
      "           Block-143             [-1, 148, 768]               0\n",
      "       LayerNorm-144             [-1, 148, 768]           1,536\n",
      "          Linear-145            [-1, 148, 2304]       1,771,776\n",
      "        Identity-146          [-1, 12, 148, 64]               0\n",
      "        Identity-147          [-1, 12, 148, 64]               0\n",
      "          Linear-148             [-1, 148, 768]         590,592\n",
      "         Dropout-149             [-1, 148, 768]               0\n",
      "       Attention-150             [-1, 148, 768]               0\n",
      "        Identity-151             [-1, 148, 768]               0\n",
      "        Identity-152             [-1, 148, 768]               0\n",
      "       LayerNorm-153             [-1, 148, 768]           1,536\n",
      "          Linear-154            [-1, 148, 3072]       2,362,368\n",
      "            GELU-155            [-1, 148, 3072]               0\n",
      "         Dropout-156            [-1, 148, 3072]               0\n",
      "        Identity-157            [-1, 148, 3072]               0\n",
      "          Linear-158             [-1, 148, 768]       2,360,064\n",
      "         Dropout-159             [-1, 148, 768]               0\n",
      "             Mlp-160             [-1, 148, 768]               0\n",
      "        Identity-161             [-1, 148, 768]               0\n",
      "        Identity-162             [-1, 148, 768]               0\n",
      "           Block-163             [-1, 148, 768]               0\n",
      "       LayerNorm-164             [-1, 148, 768]           1,536\n",
      "          Linear-165            [-1, 148, 2304]       1,771,776\n",
      "        Identity-166          [-1, 12, 148, 64]               0\n",
      "        Identity-167          [-1, 12, 148, 64]               0\n",
      "          Linear-168             [-1, 148, 768]         590,592\n",
      "         Dropout-169             [-1, 148, 768]               0\n",
      "       Attention-170             [-1, 148, 768]               0\n",
      "        Identity-171             [-1, 148, 768]               0\n",
      "        Identity-172             [-1, 148, 768]               0\n",
      "       LayerNorm-173             [-1, 148, 768]           1,536\n",
      "          Linear-174            [-1, 148, 3072]       2,362,368\n",
      "            GELU-175            [-1, 148, 3072]               0\n",
      "         Dropout-176            [-1, 148, 3072]               0\n",
      "        Identity-177            [-1, 148, 3072]               0\n",
      "          Linear-178             [-1, 148, 768]       2,360,064\n",
      "         Dropout-179             [-1, 148, 768]               0\n",
      "             Mlp-180             [-1, 148, 768]               0\n",
      "        Identity-181             [-1, 148, 768]               0\n",
      "        Identity-182             [-1, 148, 768]               0\n",
      "           Block-183             [-1, 148, 768]               0\n",
      "       LayerNorm-184             [-1, 148, 768]           1,536\n",
      "          Linear-185            [-1, 148, 2304]       1,771,776\n",
      "        Identity-186          [-1, 12, 148, 64]               0\n",
      "        Identity-187          [-1, 12, 148, 64]               0\n",
      "          Linear-188             [-1, 148, 768]         590,592\n",
      "         Dropout-189             [-1, 148, 768]               0\n",
      "       Attention-190             [-1, 148, 768]               0\n",
      "        Identity-191             [-1, 148, 768]               0\n",
      "        Identity-192             [-1, 148, 768]               0\n",
      "       LayerNorm-193             [-1, 148, 768]           1,536\n",
      "          Linear-194            [-1, 148, 3072]       2,362,368\n",
      "            GELU-195            [-1, 148, 3072]               0\n",
      "         Dropout-196            [-1, 148, 3072]               0\n",
      "        Identity-197            [-1, 148, 3072]               0\n",
      "          Linear-198             [-1, 148, 768]       2,360,064\n",
      "         Dropout-199             [-1, 148, 768]               0\n",
      "             Mlp-200             [-1, 148, 768]               0\n",
      "        Identity-201             [-1, 148, 768]               0\n",
      "        Identity-202             [-1, 148, 768]               0\n",
      "           Block-203             [-1, 148, 768]               0\n",
      "       LayerNorm-204             [-1, 148, 768]           1,536\n",
      "          Linear-205            [-1, 148, 2304]       1,771,776\n",
      "        Identity-206          [-1, 12, 148, 64]               0\n",
      "        Identity-207          [-1, 12, 148, 64]               0\n",
      "          Linear-208             [-1, 148, 768]         590,592\n",
      "         Dropout-209             [-1, 148, 768]               0\n",
      "       Attention-210             [-1, 148, 768]               0\n",
      "        Identity-211             [-1, 148, 768]               0\n",
      "        Identity-212             [-1, 148, 768]               0\n",
      "       LayerNorm-213             [-1, 148, 768]           1,536\n",
      "          Linear-214            [-1, 148, 3072]       2,362,368\n",
      "            GELU-215            [-1, 148, 3072]               0\n",
      "         Dropout-216            [-1, 148, 3072]               0\n",
      "        Identity-217            [-1, 148, 3072]               0\n",
      "          Linear-218             [-1, 148, 768]       2,360,064\n",
      "         Dropout-219             [-1, 148, 768]               0\n",
      "             Mlp-220             [-1, 148, 768]               0\n",
      "        Identity-221             [-1, 148, 768]               0\n",
      "        Identity-222             [-1, 148, 768]               0\n",
      "           Block-223             [-1, 148, 768]               0\n",
      "       LayerNorm-224             [-1, 148, 768]           1,536\n",
      "          Linear-225            [-1, 148, 2304]       1,771,776\n",
      "        Identity-226          [-1, 12, 148, 64]               0\n",
      "        Identity-227          [-1, 12, 148, 64]               0\n",
      "          Linear-228             [-1, 148, 768]         590,592\n",
      "         Dropout-229             [-1, 148, 768]               0\n",
      "       Attention-230             [-1, 148, 768]               0\n",
      "        Identity-231             [-1, 148, 768]               0\n",
      "        Identity-232             [-1, 148, 768]               0\n",
      "       LayerNorm-233             [-1, 148, 768]           1,536\n",
      "          Linear-234            [-1, 148, 3072]       2,362,368\n",
      "            GELU-235            [-1, 148, 3072]               0\n",
      "         Dropout-236            [-1, 148, 3072]               0\n",
      "        Identity-237            [-1, 148, 3072]               0\n",
      "          Linear-238             [-1, 148, 768]       2,360,064\n",
      "         Dropout-239             [-1, 148, 768]               0\n",
      "             Mlp-240             [-1, 148, 768]               0\n",
      "        Identity-241             [-1, 148, 768]               0\n",
      "        Identity-242             [-1, 148, 768]               0\n",
      "           Block-243             [-1, 148, 768]               0\n",
      "       LayerNorm-244             [-1, 148, 768]           1,536\n",
      "          Linear-245             [-1, 148, 512]         393,728\n",
      "       LayerNorm-246             [-1, 589, 512]           1,024\n",
      "          Linear-247            [-1, 589, 1536]         787,968\n",
      "        Identity-248          [-1, 16, 589, 32]               0\n",
      "        Identity-249          [-1, 16, 589, 32]               0\n",
      "          Linear-250             [-1, 589, 512]         262,656\n",
      "         Dropout-251             [-1, 589, 512]               0\n",
      "       Attention-252             [-1, 589, 512]               0\n",
      "        Identity-253             [-1, 589, 512]               0\n",
      "        Identity-254             [-1, 589, 512]               0\n",
      "       LayerNorm-255             [-1, 589, 512]           1,024\n",
      "          Linear-256            [-1, 589, 2048]       1,050,624\n",
      "            GELU-257            [-1, 589, 2048]               0\n",
      "         Dropout-258            [-1, 589, 2048]               0\n",
      "        Identity-259            [-1, 589, 2048]               0\n",
      "          Linear-260             [-1, 589, 512]       1,049,088\n",
      "         Dropout-261             [-1, 589, 512]               0\n",
      "             Mlp-262             [-1, 589, 512]               0\n",
      "        Identity-263             [-1, 589, 512]               0\n",
      "        Identity-264             [-1, 589, 512]               0\n",
      "           Block-265             [-1, 589, 512]               0\n",
      "       LayerNorm-266             [-1, 589, 512]           1,024\n",
      "          Linear-267            [-1, 589, 1536]         787,968\n",
      "        Identity-268          [-1, 16, 589, 32]               0\n",
      "        Identity-269          [-1, 16, 589, 32]               0\n",
      "          Linear-270             [-1, 589, 512]         262,656\n",
      "         Dropout-271             [-1, 589, 512]               0\n",
      "       Attention-272             [-1, 589, 512]               0\n",
      "        Identity-273             [-1, 589, 512]               0\n",
      "        Identity-274             [-1, 589, 512]               0\n",
      "       LayerNorm-275             [-1, 589, 512]           1,024\n",
      "          Linear-276            [-1, 589, 2048]       1,050,624\n",
      "            GELU-277            [-1, 589, 2048]               0\n",
      "         Dropout-278            [-1, 589, 2048]               0\n",
      "        Identity-279            [-1, 589, 2048]               0\n",
      "          Linear-280             [-1, 589, 512]       1,049,088\n",
      "         Dropout-281             [-1, 589, 512]               0\n",
      "             Mlp-282             [-1, 589, 512]               0\n",
      "        Identity-283             [-1, 589, 512]               0\n",
      "        Identity-284             [-1, 589, 512]               0\n",
      "           Block-285             [-1, 589, 512]               0\n",
      "       LayerNorm-286             [-1, 589, 512]           1,024\n",
      "          Linear-287            [-1, 589, 1536]         787,968\n",
      "        Identity-288          [-1, 16, 589, 32]               0\n",
      "        Identity-289          [-1, 16, 589, 32]               0\n",
      "          Linear-290             [-1, 589, 512]         262,656\n",
      "         Dropout-291             [-1, 589, 512]               0\n",
      "       Attention-292             [-1, 589, 512]               0\n",
      "        Identity-293             [-1, 589, 512]               0\n",
      "        Identity-294             [-1, 589, 512]               0\n",
      "       LayerNorm-295             [-1, 589, 512]           1,024\n",
      "          Linear-296            [-1, 589, 2048]       1,050,624\n",
      "            GELU-297            [-1, 589, 2048]               0\n",
      "         Dropout-298            [-1, 589, 2048]               0\n",
      "        Identity-299            [-1, 589, 2048]               0\n",
      "          Linear-300             [-1, 589, 512]       1,049,088\n",
      "         Dropout-301             [-1, 589, 512]               0\n",
      "             Mlp-302             [-1, 589, 512]               0\n",
      "        Identity-303             [-1, 589, 512]               0\n",
      "        Identity-304             [-1, 589, 512]               0\n",
      "           Block-305             [-1, 589, 512]               0\n",
      "       LayerNorm-306             [-1, 589, 512]           1,024\n",
      "          Linear-307            [-1, 589, 1536]         787,968\n",
      "        Identity-308          [-1, 16, 589, 32]               0\n",
      "        Identity-309          [-1, 16, 589, 32]               0\n",
      "          Linear-310             [-1, 589, 512]         262,656\n",
      "         Dropout-311             [-1, 589, 512]               0\n",
      "       Attention-312             [-1, 589, 512]               0\n",
      "        Identity-313             [-1, 589, 512]               0\n",
      "        Identity-314             [-1, 589, 512]               0\n",
      "       LayerNorm-315             [-1, 589, 512]           1,024\n",
      "          Linear-316            [-1, 589, 2048]       1,050,624\n",
      "            GELU-317            [-1, 589, 2048]               0\n",
      "         Dropout-318            [-1, 589, 2048]               0\n",
      "        Identity-319            [-1, 589, 2048]               0\n",
      "          Linear-320             [-1, 589, 512]       1,049,088\n",
      "         Dropout-321             [-1, 589, 512]               0\n",
      "             Mlp-322             [-1, 589, 512]               0\n",
      "        Identity-323             [-1, 589, 512]               0\n",
      "        Identity-324             [-1, 589, 512]               0\n",
      "           Block-325             [-1, 589, 512]               0\n",
      "       LayerNorm-326             [-1, 589, 512]           1,024\n",
      "          Linear-327            [-1, 589, 1536]         787,968\n",
      "        Identity-328          [-1, 16, 589, 32]               0\n",
      "        Identity-329          [-1, 16, 589, 32]               0\n",
      "          Linear-330             [-1, 589, 512]         262,656\n",
      "         Dropout-331             [-1, 589, 512]               0\n",
      "       Attention-332             [-1, 589, 512]               0\n",
      "        Identity-333             [-1, 589, 512]               0\n",
      "        Identity-334             [-1, 589, 512]               0\n",
      "       LayerNorm-335             [-1, 589, 512]           1,024\n",
      "          Linear-336            [-1, 589, 2048]       1,050,624\n",
      "            GELU-337            [-1, 589, 2048]               0\n",
      "         Dropout-338            [-1, 589, 2048]               0\n",
      "        Identity-339            [-1, 589, 2048]               0\n",
      "          Linear-340             [-1, 589, 512]       1,049,088\n",
      "         Dropout-341             [-1, 589, 512]               0\n",
      "             Mlp-342             [-1, 589, 512]               0\n",
      "        Identity-343             [-1, 589, 512]               0\n",
      "        Identity-344             [-1, 589, 512]               0\n",
      "           Block-345             [-1, 589, 512]               0\n",
      "       LayerNorm-346             [-1, 589, 512]           1,024\n",
      "          Linear-347            [-1, 589, 1536]         787,968\n",
      "        Identity-348          [-1, 16, 589, 32]               0\n",
      "        Identity-349          [-1, 16, 589, 32]               0\n",
      "          Linear-350             [-1, 589, 512]         262,656\n",
      "         Dropout-351             [-1, 589, 512]               0\n",
      "       Attention-352             [-1, 589, 512]               0\n",
      "        Identity-353             [-1, 589, 512]               0\n",
      "        Identity-354             [-1, 589, 512]               0\n",
      "       LayerNorm-355             [-1, 589, 512]           1,024\n",
      "          Linear-356            [-1, 589, 2048]       1,050,624\n",
      "            GELU-357            [-1, 589, 2048]               0\n",
      "         Dropout-358            [-1, 589, 2048]               0\n",
      "        Identity-359            [-1, 589, 2048]               0\n",
      "          Linear-360             [-1, 589, 512]       1,049,088\n",
      "         Dropout-361             [-1, 589, 512]               0\n",
      "             Mlp-362             [-1, 589, 512]               0\n",
      "        Identity-363             [-1, 589, 512]               0\n",
      "        Identity-364             [-1, 589, 512]               0\n",
      "           Block-365             [-1, 589, 512]               0\n",
      "       LayerNorm-366             [-1, 589, 512]           1,024\n",
      "          Linear-367            [-1, 589, 1536]         787,968\n",
      "        Identity-368          [-1, 16, 589, 32]               0\n",
      "        Identity-369          [-1, 16, 589, 32]               0\n",
      "          Linear-370             [-1, 589, 512]         262,656\n",
      "         Dropout-371             [-1, 589, 512]               0\n",
      "       Attention-372             [-1, 589, 512]               0\n",
      "        Identity-373             [-1, 589, 512]               0\n",
      "        Identity-374             [-1, 589, 512]               0\n",
      "       LayerNorm-375             [-1, 589, 512]           1,024\n",
      "          Linear-376            [-1, 589, 2048]       1,050,624\n",
      "            GELU-377            [-1, 589, 2048]               0\n",
      "         Dropout-378            [-1, 589, 2048]               0\n",
      "        Identity-379            [-1, 589, 2048]               0\n",
      "          Linear-380             [-1, 589, 512]       1,049,088\n",
      "         Dropout-381             [-1, 589, 512]               0\n",
      "             Mlp-382             [-1, 589, 512]               0\n",
      "        Identity-383             [-1, 589, 512]               0\n",
      "        Identity-384             [-1, 589, 512]               0\n",
      "           Block-385             [-1, 589, 512]               0\n",
      "       LayerNorm-386             [-1, 589, 512]           1,024\n",
      "          Linear-387            [-1, 589, 1536]         787,968\n",
      "        Identity-388          [-1, 16, 589, 32]               0\n",
      "        Identity-389          [-1, 16, 589, 32]               0\n",
      "          Linear-390             [-1, 589, 512]         262,656\n",
      "         Dropout-391             [-1, 589, 512]               0\n",
      "       Attention-392             [-1, 589, 512]               0\n",
      "        Identity-393             [-1, 589, 512]               0\n",
      "        Identity-394             [-1, 589, 512]               0\n",
      "       LayerNorm-395             [-1, 589, 512]           1,024\n",
      "          Linear-396            [-1, 589, 2048]       1,050,624\n",
      "            GELU-397            [-1, 589, 2048]               0\n",
      "         Dropout-398            [-1, 589, 2048]               0\n",
      "        Identity-399            [-1, 589, 2048]               0\n",
      "          Linear-400             [-1, 589, 512]       1,049,088\n",
      "         Dropout-401             [-1, 589, 512]               0\n",
      "             Mlp-402             [-1, 589, 512]               0\n",
      "        Identity-403             [-1, 589, 512]               0\n",
      "        Identity-404             [-1, 589, 512]               0\n",
      "           Block-405             [-1, 589, 512]               0\n",
      "       LayerNorm-406             [-1, 589, 512]           1,024\n",
      "          Linear-407            [-1, 589, 1536]         787,968\n",
      "================================================================\n",
      "Total params: 112,638,208\n",
      "Trainable params: 112,638,208\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.45\n",
      "Forward/backward pass size (MB): 1000.61\n",
      "Params size (MB): 429.68\n",
      "Estimated Total Size (MB): 1433.74\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laszer/.pyenv/versions/3.9.6/envs/hackathon_nasa/lib/python3.9/site-packages/torch/nn/modules/conv.py:605: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv3d(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "n_channels = 6\n",
    "temporal_length = 3\n",
    "height = 224\n",
    "width = 224\n",
    "input_size = (n_channels, temporal_length, height, width)\n",
    "summary(model, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nasa_hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
